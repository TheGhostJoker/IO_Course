{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), this is just a very basic implementation.\n",
    "\n",
    "Here we will train the model on the iris dataset. We have worked with our own data in the previous example, and here we will work with a dataset that comes with scikit learn (the iris dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]] [0 0 0 0 0]\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(X[:5], y[:5])\n",
    "#printing the first five rows of the data to look at how the data is structured \n",
    "print(set(y))\n",
    "#to get the possible values that y can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have four features in every data point, and a single label that we need to fit our model to. But our output variable y can take 3 values, so this is not a binary classification problem (where we only decide between two classes). so we set ```multi_class``` parameter to ```\"multinomial\"``` (as we have multiple classes).\n",
    "\n",
    "```lbfgs``` is the optimization algorithm that we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter = 1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y) # this is the mean accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction\n",
    "\n",
    "We get the prediction by the ```predict()``` function. We use ```predict_proba()``` to get the class-wise probability estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  [0]\n",
      "prob estimates:  [[9.95258584e-01 4.74141073e-03 5.69420118e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction: \", clf.predict([[4.3, 3.8, 1.3, 0.3]]))\n",
    "\n",
    "print(\"prob estimates: \", clf.predict_proba([[4.3, 3.8, 1.3, 0.3]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how the predicted class is the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
