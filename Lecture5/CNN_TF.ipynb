{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks using Tensorflow\n",
    "\n",
    "\n",
    "Note: most of the explanation remains the same.\n",
    "\n",
    "\n",
    "Here, we will be using Tensorflow and keras to build our neural network. Keras is now used as a part of tensorflow (could be used as a standalone package before, but this is better).\n",
    "\n",
    "Here, we will be working with the MNIST dataset that contains 10's of thousands of black and white images of handwritten numbers. We will be training our model to predict what the written number is.\n",
    "\n",
    "Note: Don't try to memorize the syntax and functions. Make sure you understand what is going on though.\n",
    "You will learn the syntax as you practice more problems. [This](https://www.tensorflow.org/api_docs) is the link to TensorFlow documentation. Be careful about the versions though, TF2.0 is fairly different from this version TF 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This dataset is probably the most common dataset used, and hence, it comes preloaded in keras. As we can see, it is split into images and labels, and further into train and test. \n",
    "\n",
    "The labels contain the actual number that is written in the image.\n",
    "\n",
    "In the following steps, we see that there are 60000 training images, each of the size 28 pixels (height) and 28 pixels (width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "It is recommended that you normalize your input before sending it to your neural network. A short answer on why from [this](https://www.quora.com/Why-does-mean-normalization-help-in-gradient-descent) Quora post:\n",
    "\n",
    "\"The problem is when you have features with different scales. A commonly used optimization technique, such as gradient descent, uses the product of the learning rate times the gradient of the cost function as the step size. As a result, when the features have different scales, the step size can be different for each feature. When you try different learning rates, you will find that 1) the learning rate is too small and it will take a long time to converge, or 2) the learning rate is too big for one or more features, and it never converges.\"\n",
    "\n",
    "\n",
    "In images, the highest pixel value is 255, and lowest is 0. This is a black and white image (grayscale) so images have only one channel (as opposed to Red, Blue, Green channels (3) in full color images).\n",
    "\n",
    "To scale the values to some value between 0 and 1, one method is to divide every pixel value by 255. And this is what we do in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reshaping inputs\n",
    "\n",
    "We need to reshape the data into a certain format, for conv2d to be able to work with it. In case we skip the reshaping step, we will encounter errors.\n",
    "More details [here](https://stackoverflow.com/questions/37085653/when-bulding-a-cnn-i-am-getting-complaints-from-keras-that-do-not-make-sense-to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "train_images = train_images.reshape([-1, 28, 28, 1])\n",
    "test_images = test_images.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the convolutional neural network\n",
    "\n",
    "We start off with a convolutional layer with 32 filters each of dimension 3 x 3. The activation used is ```relu``` (you can experiment with other activations like ```tanh``` too, and see what happens) and we also define the input shape.\n",
    "For more information, look at the link at the end. Even though its on Pytorch, it could give you some insights on how CNNs work.\n",
    "\n",
    "After that we have a maxpooling layer with a pool size of 2x2. Then a dropout layer with a drop percentage of 25%.\n",
    "Then we flatten and pass it on to a fully connected layer of size 10, which is also the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential ([\n",
    "    keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28, 28, 1)),\n",
    "    keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ```ADAM``` optimizer which another optimizer like gradient descent with a ```sparse_categorical_crossentropy``` loss (this is a solid choice because we are dealing with categorical data).  (you can read more about these at the link provided at the end of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We use ```model.fit(input_data, target_labels)``` to train our model on input data to fit the target labels. We are going to train for five epoch. Each epoch signifies one iteration over the whole training data.\n",
    "\n",
    "Due to extra parameters in the convolutional layers, this will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 46s 766us/sample - loss: 0.2317 - acc: 0.9337\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 51s 849us/sample - loss: 0.0977 - acc: 0.9708\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 48s 805us/sample - loss: 0.0771 - acc: 0.9765\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 47s 785us/sample - loss: 0.0660 - acc: 0.9801\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 51s 845us/sample - loss: 0.0578 - acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff502ff07f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "\n",
    "We test our model on the test images and measure its performance by comparing its predictions with the actual test labels.\n",
    "We see that it gives 98.18% accuracy, greater than what we previously acheived with an ANN, by just adding a single convolutional layer. This might not seem like much, but for bigger problems, CNNs will give significantly much better results.\n",
    "\n",
    "You can experiment by adding more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0548 - acc: 0.9818\n",
      "0.9818\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Implementation\n",
    "\n",
    "[This](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) blog post has a Pytorch implementation of a convolutional neural network along with detailed explanation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
