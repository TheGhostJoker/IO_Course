{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks using Tensorflow\n",
    "\n",
    "Here, we will be using Tensorflow and keras to build our neural network. Keras is now used as a part of tensorflow (could be used as a standalone package before, but this is better).\n",
    "\n",
    "Here, we will be working with the MNIST dataset that contains 10's of thousands of black and white images of handwritten numbers. We will be training our model to predict what the written number is.\n",
    "\n",
    "Note: Don't try to memorize the syntax and functions. Make sure you understand what is going on though.\n",
    "You will learn the syntax as you practice more problems. [This](https://www.tensorflow.org/api_docs) is the link to TensorFlow documentation. Be careful about the versions though, TF2.0 is fairly different from this version TF 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This dataset is probably the most common dataset used, and hence, it comes preloaded in keras. As we can see, it is split into images and labels, and further into train and test. \n",
    "\n",
    "The labels contain the actual number that is written in the image.\n",
    "\n",
    "In the following steps, we see that there are 60000 training images, each of the size 28 pixels (height) and 28 pixels (width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 98s 9us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "It is recommended that you normalize your input before sending it to your neural network. A short answer on why from [this](https://www.quora.com/Why-does-mean-normalization-help-in-gradient-descent) Quora post:\n",
    "\n",
    "\"The problem is when you have features with different scales. A commonly used optimization technique, such as gradient descent, uses the product of the learning rate times the gradient of the cost function as the step size. As a result, when the features have different scales, the step size can be different for each feature. When you try different learning rates, you will find that 1) the learning rate is too small and it will take a long time to converge, or 2) the learning rate is too big for one or more features, and it never converges.\"\n",
    "\n",
    "\n",
    "In images, the highest pixel value is 255, and lowest is 0. This is a black and white image (grayscale) so images have only one channel (as opposed to Red, Blue, Green channels (3) in full color images).\n",
    "\n",
    "To scale the values to some value between 0 and 1, one method is to divide every pixel value by 255. And this is what we do in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "We will build a neural network with one input layer, one hidden layer, and one output layer. \n",
    "First we flatten the image so we get a vector and not a 2 dimensional image matrix. Here, we need to have 28 x 28 input neurons. Then we have a hidden layer with 128 neurons (it could be any number, there is no real way to tell which is the right size of neurons). And then we have 10 output neurons (because we have ten possible digits (0 - 9)) with the softmax activation. It gives us the number that the image is most likely to be. \n",
    "\n",
    "More information is provided in a link at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential ([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation = tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the ```ADAM``` optimizer which another optimizer like gradient descent with a ```sparse_categorical_crossentropy``` loss (this is a solid choice because we are dealing with categorical data).  (you can read more about these at the link provided at the end of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We use ```model.fit(input_data, target_labels)``` to train our model on input data to fit the target labels. We are going to train for five epoch. Each epoch signifies one iteration over the whole training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 12s 206us/sample - loss: 0.2562 - acc: 0.9265\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.1119 - acc: 0.9666\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0765 - acc: 0.9770\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 12s 202us/sample - loss: 0.0579 - acc: 0.9823\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0446 - acc: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7a9df8bba8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model\n",
    "\n",
    "We test our model on the test images and measure its performance by comparing its predictions with the actual test labels.\n",
    "We see that it gives 97.7% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0743 - acc: 0.9770\n",
      "0.977\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More information\n",
    "\n",
    "go here to see a more detailed explanation on a similiar architecture but on a different dataset. https://www.tensorflow.org/tutorials/keras/basic_classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
