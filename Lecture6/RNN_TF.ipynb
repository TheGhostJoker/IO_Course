{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_TF.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjcvyYxfpJDb",
        "colab_type": "text"
      },
      "source": [
        "# RNN using tensorflow\n",
        "\n",
        "Here we will be trying to classify movie reviews as positive or negative based on 50,000 reviews from imdb. We will have 25,000 in the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K4j8V0OpJDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PgXwIWzpJDr",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset comes preloaded in imdb.\n",
        "\n",
        "Like before, we load the train data and test data.\n",
        "\n",
        "We only include the top 10,000 words. This is because we can do most of the predicting using only the most frequently occuring words, and the rare words do not contribute much.\n",
        "\n",
        "Every word is mapped to an integer, which serves as the index. So using this, we can get back the word we initially had. \n",
        "More details on this at the end of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H5UzccrpJDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNPZkgZLpJDz",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "In tensorflow, we need all sequences to be of a fixed length(i.e., same number of timesteps). \n",
        "\n",
        "However, sentences in general are bound to have varying numbers of words. So as a work around to this, we pad every sentence/review to be of the same length. In this case, every input is padded to a length of 256. The model simply ignore these values when we train it.\n",
        "\n",
        "Here, we are padding with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhMmxQeWpJD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7E-kMNhpJD5",
        "colab_type": "text"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Vocab size is the number of distinct words.\n",
        "\n",
        "First off, we start with an embedding layer. And then to ignore the padded parts of the sequence, we use the masking layer. Then we have a layer with 100 LSTM neurons. Then we have a fully connected layer with 16 neurons, followed by a single neuron that outputs a value between 0 and 1.\n",
        "\n",
        "Read the first section of this [post](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) on the PyTorch documentations for an explanation on embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Vnzs39PepJD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3c70ef64-758c-43b0-addf-91d39d141a43"
      },
      "source": [
        "vocab_size = 10000\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 100),\n",
        "    keras.layers.Masking(mask_value=0., input_shape=(256, 100)),\n",
        "    keras.layers.LSTM(100),\n",
        "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid) \n",
        "])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "masking_1 (Masking)          (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                1616      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,082,033\n",
            "Trainable params: 1,082,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbHWbFMpJEB",
        "colab_type": "text"
      },
      "source": [
        "We are using ```binary cross entropy``` as the loss function we are dealing with binary classification (positive or negative review). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaNmgwnRpJEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOhvhVzPpJEJ",
        "colab_type": "text"
      },
      "source": [
        "For the sake of time, I am only going to be using the first 10000 records. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R40Ef36IpJEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "partial_x_train = train_data[10000:]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGn2iUEypJEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "8c839cae-e38c-40b5-b516-eb9fdd8ff218"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=1024,\n",
        "                    verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/5\n",
            "15000/15000 [==============================] - 9s 627us/sample - loss: 0.6928 - acc: 0.5217\n",
            "Epoch 2/5\n",
            "15000/15000 [==============================] - 9s 577us/sample - loss: 0.6864 - acc: 0.5466\n",
            "Epoch 3/5\n",
            "15000/15000 [==============================] - 8s 562us/sample - loss: 0.6237 - acc: 0.6550\n",
            "Epoch 4/5\n",
            "15000/15000 [==============================] - 8s 562us/sample - loss: 0.4531 - acc: 0.8215\n",
            "Epoch 5/5\n",
            "15000/15000 [==============================] - 8s 559us/sample - loss: 0.3615 - acc: 0.8766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9rrVToepJEU",
        "colab_type": "text"
      },
      "source": [
        "#### Testing\n",
        "\n",
        "We see that using this approach gets us an accuracy of 83.57%. We can improve this by training it for longer and using a better architecture. Feel free to experiment around with the architecture, but make sure to get your dimensions right. And for that, always refer to the docs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhSaHh7dpJEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "67fe403e-be6a-4cd6-c5b2-64593a821fad"
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 127s 5ms/sample - loss: 0.4463 - acc: 0.8357\n",
            "[0.44628088002204896, 0.83572]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZMH3PvpJEY",
        "colab_type": "text"
      },
      "source": [
        "This implementation is based on [this](https://www.tensorflow.org/tutorials/keras/basic_text_classification) tutorial by TF. Read it for more explanation.\n",
        "\n",
        "[This](https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17) has more methods of doing it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjqhH5MDpJEZ",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch implementation\n",
        "\n",
        "Refer to [this](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) post for the pytorch implementation of a similar problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIXsvlcOpJEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}